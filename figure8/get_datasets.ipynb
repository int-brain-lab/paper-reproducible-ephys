{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_features import *\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = ONE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_number_map = get_lab_number_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_GPIO, no_GPIO = get_valid_eids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = get_traj(has_GPIO, no_GPIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_directory = os.path.join('.', \"new_timestamps\")\n",
    "if not os.path.exists(ts_directory):\n",
    "    os.makedirs(ts_directory)\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    eid = traj[i]['session']['id']\n",
    "    try:\n",
    "        get_new_timestamps(eid, 'left', ts_directory)\n",
    "        get_new_timestamps(eid, 'right', ts_directory)\n",
    "    except:\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acronym_dict = get_acronym_dict(one, traj, has_GPIO, no_GPIO, ts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acronym_dict = np.load('../acronym_dict_new.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "output_list = []\n",
    "trial_length_array_list = []\n",
    "cluster_number_list = []\n",
    "session_list = []\n",
    "nan_trials = []\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    feature, output, trial_length_array, cluster_numbers, nan_idx, success = featurize(i, traj[i], one, lab_number_map, \n",
    "                                                                                       acronym_dict.tolist(), has_GPIO, no_GPIO, ts_directory)\n",
    "    if not success:\n",
    "        print('not successful')\n",
    "        continue\n",
    "    else:\n",
    "        print('successful')\n",
    "    feature_list.append(feature)\n",
    "    output_list.append(output)\n",
    "    trial_length_array_list.append(trial_length_array)\n",
    "    cluster_number_list.append(cluster_numbers)\n",
    "    session_list.append(traj[i])\n",
    "    nan_trials.append(nan_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feature_list)):\n",
    "    print(feature_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output_list)):\n",
    "    print(output_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_list = []\n",
    "for i in range(len(output_list)):\n",
    "    mean_fr = output_list[i].mean(2).mean(0)\n",
    "    top_k = mean_fr.argsort()[::-1][:250]\n",
    "    topk_list.append(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_feature_list = []\n",
    "for i in range(len(feature_list)):\n",
    "    top_k = topk_list[i]\n",
    "    feature = feature_list[i][:,top_k]\n",
    "    for k in range(10):\n",
    "        feature[k,:,:,0] = 10*i + k\n",
    "    truncated_feature_list.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_feature = np.asarray(truncated_feature_list)\n",
    "print(truncated_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('truncated_feature_original.npy', truncated_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen_list = []\n",
    "for i in range(len(trial_length_array_list)):\n",
    "    top_k = topk_list[i]\n",
    "    seqlen_list.append(trial_length_array_list[i][top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqlen_array = np.asarray(seqlen_list)\n",
    "print(seqlen_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('seqlen.npy', seqlen_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize truncated_feature: dlc features + xyz location\n",
    "# first normalize xyz\n",
    "x_max = truncated_feature[:,:,:,:,xyz_offset].max()\n",
    "x_min = truncated_feature[:,:,:,:,xyz_offset].min()\n",
    "y_max = truncated_feature[:,:,:,:,xyz_offset+1].max()\n",
    "y_min = truncated_feature[:,:,:,:,xyz_offset+1].min()\n",
    "z_max = truncated_feature[:,:,:,:,xyz_offset+2].max()\n",
    "z_min = truncated_feature[:,:,:,:,xyz_offset+2].min()\n",
    "\n",
    "truncated_feature[:,:,:,:,xyz_offset] = 0.1 + 0.9*(truncated_feature[:,:,:,:,xyz_offset] - x_min) / (x_max - x_min)\n",
    "truncated_feature[:,:,:,:,xyz_offset+1] = 0.1 + 0.9*(truncated_feature[:,:,:,:,xyz_offset+1] - y_min) / (y_max - y_min)\n",
    "truncated_feature[:,:,:,:,xyz_offset+2] = 0.1 + 0.9*(truncated_feature[:,:,:,:,xyz_offset+2] - z_min) / (z_max - z_min)\n",
    "\n",
    "# next normalize dlc features\n",
    "for i in range(stimOnOff_offset - left_dlc_offset):\n",
    "    idx = left_dlc_offset+i\n",
    "    \n",
    "    feature_min = truncated_feature[:,:,:,:,idx].min()\n",
    "    feature_max = truncated_feature[:,:,:,:,idx].max()\n",
    "    \n",
    "    truncated_feature[:,:,:,:,idx] = -1 + 2*(truncated_feature[:,:,:,:,idx] - feature_min) / (feature_max - feature_min)\n",
    "    \n",
    "# next normalize wheel\n",
    "wheel_min = truncated_feature[:,:,:,:,wheel_offset].min()\n",
    "wheel_max = truncated_feature[:,:,:,:,wheel_offset].max()\n",
    "\n",
    "truncated_feature[:,:,:,:,wheel_offset] = -1 + 2*(truncated_feature[:,:,:,:,wheel_offset] - wheel_min) / (wheel_max - wheel_min)\n",
    "\n",
    "\n",
    "# next normalize lick\n",
    "lick_min = truncated_feature[:,:,:,:,lick_offset].min()\n",
    "lick_max = truncated_feature[:,:,:,:,lick_offset].max()\n",
    "\n",
    "truncated_feature[:,:,:,:,lick_offset] = (truncated_feature[:,:,:,:,lick_offset] - lick_min) / (lick_max - lick_min)\n",
    "\n",
    "# next normalize max_ptp\n",
    "max_ptp_min = truncated_feature[:,:,:,:,max_ptp_offset].min()\n",
    "max_ptp_max = truncated_feature[:,:,:,:,max_ptp_offset].max()\n",
    "\n",
    "truncated_feature[:,:,:,:,max_ptp_offset] = 0.1 + 0.9*(truncated_feature[:,:,:,:,max_ptp_offset] - max_ptp_min) / (max_ptp_max - max_ptp_min)\n",
    "\n",
    "# next normalize wf_width\n",
    "wf_width_min = truncated_feature[:,:,:,:,wf_width_offset].min()\n",
    "wf_width_max = truncated_feature[:,:,:,:,wf_width_offset].max()\n",
    "\n",
    "truncated_feature[:,:,:,:,wf_width_offset] = 0.1 + 0.9*(truncated_feature[:,:,:,:,wf_width_offset] - wf_width_min) / (wf_width_max - wf_width_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('truncated_feature_normalized.npy', truncated_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_target_list = []\n",
    "for i in range(len(output_list)):\n",
    "    top_k = topk_list[i]\n",
    "    output = output_list[i][:,top_k]\n",
    "    truncated_target_list.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_target = np.asarray(truncated_target_list)\n",
    "print(truncated_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('truncated_output.npy', truncated_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "iblenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
