{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import brainbox.io.one as bbone\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from reproducible_ephys_functions import query\n",
    "from tqdm import notebook\n",
    "import brainbox as bb\n",
    "from ibllib.io import spikeglx\n",
    "import brainbox.behavior.wheel as wh\n",
    "from brainbox import singlecell\n",
    "from brainbox.metrics.single_units import spike_sorting_metrics\n",
    "from one.api import ONE\n",
    "import alf\n",
    "\n",
    "from ibllib.qc.camera import CameraQC\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "\n",
    "from utils import *\n",
    "\n",
    "one = ONE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mtnn_criteria = check_mtnn_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load all rs trajectories\n",
    "# trajs = query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for eid in mtnn_criteria.keys():\n",
    "#     print(eid)\n",
    "#     print(mtnn_criteria[eid])\n",
    "#     for traj in trajs:\n",
    "#         if traj['session']['id'] == eid:\n",
    "#             print(traj)\n",
    "#             print(traj['session']['start_time'].split('T')[0]+'-'+str(traj['session']['number']))\n",
    "#     print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for eid in get_mtnn_eids().keys():\n",
    "#     print(eid)\n",
    "\n",
    "#     stimOn_times = one.load_dataset(eid, '_ibl_trials.stimOn_times.npy')\n",
    "#     firstMovement_times = one.load_dataset(eid, '_ibl_trials.firstMovement_times.npy')\n",
    "#     #print(np.nanmin(np.diff(firstMovement_times, prepend=0)))\n",
    "\n",
    "#     goCue_times = one.load_dataset(eid, '_ibl_trials.goCue_times.npy')\n",
    "\n",
    "#     choice = one.load_dataset(eid, '_ibl_trials.choice.npy')\n",
    "#     response_times = one.load_dataset(eid, '_ibl_trials.response_times.npy')\n",
    "\n",
    "#     feedbackType = one.load_dataset(eid, '_ibl_trials.feedbackType.npy')\n",
    "#     feedback_times = one.load_dataset(eid, '_ibl_trials.feedback_times.npy')\n",
    "\n",
    "#     stimOff_times = one.load_dataset(eid, '_ibl_trials.stimOff_times.npy')\n",
    "    \n",
    "#     stimOff_diff = np.diff(stimOff_times, prepend=0)\n",
    "    \n",
    "#     choice_filter = np.where(choice!=0)\n",
    "\n",
    "#     stimOn_times = stimOn_times[choice_filter]\n",
    "#     firstMovement_times = firstMovement_times[choice_filter]\n",
    "#     goCue_times = goCue_times[choice_filter]\n",
    "#     response_times = response_times[choice_filter]\n",
    "#     choice = choice[choice_filter]\n",
    "#     feedbackType = feedbackType[choice_filter]\n",
    "#     feedback_times = feedback_times[choice_filter]\n",
    "#     stimOff_times = stimOff_times[choice_filter]\n",
    "#     stimOff_diff = stimOff_diff[choice_filter]\n",
    "\n",
    "#     nan_idx = set()\n",
    "#     nan_idx.update(np.where(np.isnan(stimOn_times))[0].tolist())\n",
    "#     nan_idx.update(np.where(np.isnan(firstMovement_times))[0].tolist())\n",
    "#     nan_idx.update(np.where(np.isnan(goCue_times))[0].tolist())\n",
    "#     nan_idx.update(np.where(np.isnan(response_times))[0].tolist())\n",
    "#     nan_idx.update(np.where(np.isnan(feedback_times))[0].tolist())\n",
    "#     nan_idx.update(np.where(np.isnan(stimOff_times))[0].tolist())\n",
    "#     nan_idx = list(nan_idx)\n",
    "\n",
    "#     kept_idx = np.ones(stimOn_times.shape[0]).astype(bool)\n",
    "#     kept_idx[nan_idx] = False\n",
    "    \n",
    "#     stimOn_times = stimOn_times[kept_idx]\n",
    "#     firstMovement_times = firstMovement_times[kept_idx]\n",
    "#     goCue_times = goCue_times[kept_idx]\n",
    "#     response_times = response_times[kept_idx]\n",
    "#     choice = choice[kept_idx]\n",
    "#     feedbackType = feedbackType[kept_idx]\n",
    "#     feedback_times = feedback_times[kept_idx]\n",
    "#     stimOff_times = stimOff_times[kept_idx]\n",
    "#     stimOff_diff = stimOff_diff[kept_idx]\n",
    "\n",
    "# #     diff = firstMovement_times-stimOn_times\n",
    "# #     diff_idx = np.logical_and(diff > 0.0, diff < 1.0)\n",
    "# #     diff = diff[diff_idx]\n",
    "# #     print('max: {}, min: {}'.format(diff.max(), diff.min()))\n",
    "# #     feedback_times = feedback_times[diff_idx]\n",
    "# #     firstMovement_times = firstMovement_times[diff_idx]\n",
    "    \n",
    "# #     diff2 = feedback_times - firstMovement_times\n",
    "# #     diff2_idx = np.logical_and(diff2 > 0.0, diff2 < 1.0)\n",
    "# #     diff2 = diff2[diff2_idx]\n",
    "# #     print('max: {}, min: {}'.format(diff2.max(), diff2.min()))\n",
    "    \n",
    "# #     print(diff2.shape)\n",
    "#     stimOff_diff = stimOff_diff[np.where(stimOff_diff<10)]\n",
    "#     print(stimOff_diff.shape)\n",
    "\n",
    "#     print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtnn_eids = get_mtnn_eids()\n",
    "print(list(mtnn_eids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Charles's behavioral model\n",
    "# run_exp_prevAction(mtnn_eids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = get_traj(mtnn_eids)\n",
    "#print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ts_directory = os.path.join('.', \"new_timestamps\")\n",
    "# if not os.path.exists(ts_directory):\n",
    "#     os.makedirs(ts_directory)\n",
    "# for i in notebook.tqdm(range(len(traj))):\n",
    "#     eid = traj[i]['session']['id']\n",
    "#     get_new_timestamps(eid, 'left', ts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "output_list = []\n",
    "cluster_number_list = []\n",
    "trial_number_list = []\n",
    "session_list = []\n",
    "session_count = {'mainenlab': 0, 'churchlandlab': 0, \n",
    "                 ('hoferlab', 'mrsicflogellab'): 0, \n",
    "                 'danlab': 0, 'angelakilab':0}\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    feature, output, cluster_numbers, trial_numbers = featurize(i, traj[i], one, session_count)\n",
    "    feature_list.append(feature)\n",
    "    output_list.append(output)\n",
    "    cluster_number_list.append(cluster_numbers)\n",
    "    session_list.append(traj[i])\n",
    "    trial_number_list.append(trial_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feature_list)):\n",
    "    print(session_list[i]['session']['id'])\n",
    "    print(feature_list[i].shape)\n",
    "    np.save('./original_data/{}_feature.npy'.format(session_list[i]['session']['id']), feature_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output_list)):\n",
    "    print(session_list[i]['session']['id'])\n",
    "    print(output_list[i].shape)\n",
    "    np.save('./original_data/{}_output.npy'.format(session_list[i]['session']['id']), output_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(cluster_number_list)):\n",
    "    print(session_list[i]['session']['id'])\n",
    "    print(cluster_number_list[i].shape)\n",
    "    np.save('./original_data/{}_clusters.npy'.format(session_list[i]['session']['id']), cluster_number_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(session_list)):\n",
    "    print(session_list[i]['session']['id'])\n",
    "    np.save('./original_data/{}_session_info.npy'.format(session_list[i]['session']['id']), session_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trial_number_list)):\n",
    "    print(session_list[i]['session']['id'])\n",
    "    print(trial_number_list[i].shape)\n",
    "    np.save('./original_data/{}_trials.npy'.format(session_list[i]['session']['id']), trial_number_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_responsive_neurons(feature, output, clusters, neuron_id_start=0, threshold=5.0):\n",
    "    select = output.mean(1).max(1) >= threshold\n",
    "    feature_subset = feature[select]\n",
    "    for i in range(feature_subset.shape[0]):\n",
    "        feature_subset[i,:,:,0] = neuron_id_start+i\n",
    "    \n",
    "    return feature_subset, output[select], clusters[select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_n_neurons = 0\n",
    "shape_list = []\n",
    "output_subset_list = []\n",
    "cluster_subset_list = []\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    feature_subset, output_subset, clusters_subset = select_responsive_neurons(feature_list[i], \n",
    "                                                                               output_list[i],\n",
    "                                                                               cluster_number_list[i],\n",
    "                                                                               neuron_id_start=total_n_neurons,\n",
    "                                                                               threshold=5.0)\n",
    "    total_n_neurons += feature_subset.shape[0]\n",
    "    print('{}/{} removed'.format(feature_subset.shape[0],feature_list[i].shape[0]))\n",
    "    print('{}/{} remaining'.format(feature_list[i].shape[0]-feature_subset.shape[0],feature_list[i].shape[0]))\n",
    "    shape_list.append(feature_subset.shape)\n",
    "    output_subset_list.append(output_subset)\n",
    "    cluster_subset_list.append(clusters_subset)\n",
    "    \n",
    "    if i == 0:\n",
    "        feature_concat = feature_subset.reshape((-1,)+feature_subset.shape[-2:])\n",
    "    else:\n",
    "        feature_concat = np.concatenate((feature_concat, feature_subset.reshape((-1,)+feature_subset.shape[-2:])))\n",
    "print('feature_concat shape: {}'.format(feature_concat.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feature(feature_concat):\n",
    "    # normalize truncated_feature: dlc features + xyz location\n",
    "    # first normalize xyz\n",
    "    x_max = feature_concat[:,:,xyz_offset].max()\n",
    "    x_min = feature_concat[:,:,xyz_offset].min()\n",
    "    y_max = feature_concat[:,:,xyz_offset+1].max()\n",
    "    y_min = feature_concat[:,:,xyz_offset+1].min()\n",
    "    z_max = feature_concat[:,:,xyz_offset+2].max()\n",
    "    z_min = feature_concat[:,:,xyz_offset+2].min()\n",
    "\n",
    "    feature_concat[:,:,xyz_offset] = 0.1 + 0.9*(feature_concat[:,:,xyz_offset] - x_min) / (x_max - x_min)\n",
    "    feature_concat[:,:,xyz_offset+1] = 0.1 + 0.9*(feature_concat[:,:,xyz_offset+1] - y_min) / (y_max - y_min)\n",
    "    feature_concat[:,:,xyz_offset+2] = 0.1 + 0.9*(feature_concat[:,:,xyz_offset+2] - z_min) / (z_max - z_min)\n",
    "\n",
    "    # next normalize dlc features\n",
    "    for i in range(stimulus_offset - paw_offset):\n",
    "        idx = paw_offset+i\n",
    "\n",
    "        feature_min = feature_concat[:,:,idx].min()\n",
    "        feature_max = feature_concat[:,:,idx].max()\n",
    "\n",
    "        feature_concat[:,:,idx] = -1 + 2*(feature_concat[:,:,idx] - feature_min) / (feature_max - feature_min)\n",
    "\n",
    "    # next normalize wheel\n",
    "    wheel_min = feature_concat[:,:,wheel_offset].min()\n",
    "    wheel_max = feature_concat[:,:,wheel_offset].max()\n",
    "\n",
    "    feature_concat[:,:,wheel_offset] = -1 + 2*(feature_concat[:,:,wheel_offset] - wheel_min) / (wheel_max - wheel_min)\n",
    "\n",
    "    # next normalize max_ptp\n",
    "    max_ptp_min = feature_concat[:,:,max_ptp_offset].min()\n",
    "    max_ptp_max = feature_concat[:,:,max_ptp_offset].max()\n",
    "\n",
    "    feature_concat[:,:,max_ptp_offset] = 0.1 + 0.9*(feature_concat[:,:,max_ptp_offset] - max_ptp_min) / (max_ptp_max - max_ptp_min)\n",
    "\n",
    "    # next normalize wf_width\n",
    "    wf_width_min = feature_concat[:,:,wf_width_offset].min()\n",
    "    wf_width_max = feature_concat[:,:,wf_width_offset].max()\n",
    "\n",
    "    feature_concat[:,:,wf_width_offset] = 0.1 + 0.9*(feature_concat[:,:,wf_width_offset] - wf_width_min) / (wf_width_max - wf_width_min)\n",
    "    \n",
    "    # noise\n",
    "    noise = np.random.normal(size=feature_concat.shape[:-1])\n",
    "    feature_concat[:,:,noise_offset] = noise\n",
    "    \n",
    "    return feature_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_feature = preprocess_feature(feature_concat)\n",
    "print(preprocessed_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_feature_list = []\n",
    "idx = 0\n",
    "for sh in shape_list:\n",
    "    n = sh[0]*sh[1]\n",
    "    preprocessed_feature_list.append(preprocessed_feature[idx:idx+n].reshape(sh))\n",
    "    idx += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape_list = []\n",
    "val_shape_list = []\n",
    "test_shape_list = []\n",
    "\n",
    "train_bool_list = []\n",
    "val_bool_list = []\n",
    "test_bool_list = []\n",
    "\n",
    "train_trial_list = []\n",
    "val_trial_list = []\n",
    "test_trial_list = []\n",
    "\n",
    "train_feature = []\n",
    "val_feature = []\n",
    "test_feature = []\n",
    "\n",
    "train_output = []\n",
    "val_output = []\n",
    "test_output = []\n",
    "\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    print(session_list[i]['session']['id'])\n",
    "    \n",
    "    n_trials = preprocessed_feature_list[i].shape[1]\n",
    "    n_test = int(n_trials*0.2)\n",
    "    n_train = int((n_trials-n_test)*0.8)\n",
    "    n_val = n_trials - n_train - n_test\n",
    "    \n",
    "    sh = shape_list[i]\n",
    "    train_shape_list.append((sh[0],n_train,)+sh[-2:])\n",
    "    val_shape_list.append((sh[0],n_val,)+sh[-2:])\n",
    "    test_shape_list.append((sh[0],n_test,)+sh[-2:])\n",
    "    \n",
    "    test_idx = np.random.choice(np.arange(n_trials), size=n_test, replace=False)\n",
    "    test_bool = np.zeros(n_trials).astype(bool)\n",
    "    test_bool[test_idx] = True\n",
    "    \n",
    "    train_idx = np.random.choice(np.arange(n_trials)[~test_bool], size=n_train, replace=False)\n",
    "    train_bool = np.zeros(n_trials).astype(bool)\n",
    "    train_bool[train_idx] = True\n",
    "    \n",
    "    val_bool = np.zeros(n_trials).astype(bool)\n",
    "    val_bool[~np.logical_or(test_bool, train_bool)] = True\n",
    "    \n",
    "    train_bool_list.append(train_bool)\n",
    "    val_bool_list.append(val_bool)\n",
    "    test_bool_list.append(test_bool)\n",
    "    \n",
    "    train_trial_list.append(trial_number_list[i][train_bool])\n",
    "    val_trial_list.append(trial_number_list[i][val_bool])\n",
    "    test_trial_list.append(trial_number_list[i][test_bool])\n",
    "    \n",
    "    train_feature.append(preprocessed_feature_list[i][:,train_bool].reshape((-1,)+sh[-2:]))\n",
    "    val_feature.append(preprocessed_feature_list[i][:,val_bool].reshape((-1,)+sh[-2:]))\n",
    "    test_feature.append(preprocessed_feature_list[i][:,test_bool].reshape((-1,)+sh[-2:]))\n",
    "    \n",
    "    train_output.append(output_subset_list[i][:,train_bool].reshape(-1, sh[-2]))\n",
    "    val_output.append(output_subset_list[i][:,val_bool].reshape(-1, sh[-2]))\n",
    "    test_output.append(output_subset_list[i][:,test_bool].reshape(-1, sh[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('mtnn_data/train/shape.npy', np.asarray(train_shape_list))\n",
    "# np.save('mtnn_data/validation/shape.npy', np.asarray(val_shape_list))\n",
    "# np.save('mtnn_data/test/shape.npy', np.asarray(test_shape_list))\n",
    "\n",
    "# np.save('mtnn_data/train/bool.npy', np.asarray(train_bool_list, dtype=object))\n",
    "# np.save('mtnn_data/validation/bool.npy', np.asarray(val_bool_list, dtype=object))\n",
    "# np.save('mtnn_data/test/bool.npy', np.asarray(test_bool_list, dtype=object))\n",
    "\n",
    "# np.save('mtnn_data/train/trials.npy', np.asarray(train_trial_list, dtype=object))\n",
    "# np.save('mtnn_data/validation/trials.npy', np.asarray(val_trial_list, dtype=object))\n",
    "# np.save('mtnn_data/test/trials.npy', np.asarray(test_trial_list, dtype=object))\n",
    "\n",
    "# np.save('mtnn_data/train/feature.npy', np.concatenate(train_feature))\n",
    "# np.save('mtnn_data/validation/feature.npy', np.concatenate(val_feature))\n",
    "# np.save('mtnn_data/test/feature.npy', np.concatenate(test_feature))\n",
    "\n",
    "# np.save('mtnn_data/train/output.npy', np.concatenate(train_output))\n",
    "# np.save('mtnn_data/validation/output.npy', np.concatenate(val_output))\n",
    "# np.save('mtnn_data/test/output.npy', np.concatenate(test_output))\n",
    "\n",
    "np.save('mtnn_data/session_info.npy', np.asarray(session_list))\n",
    "np.save('mtnn_data/clusters.npy', np.asarray(cluster_subset_list, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv-updated",
   "language": "python",
   "name": "iblenv-updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
