{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from os.path import join\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import brainbox.io.one as bbone\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "from reproducible_ephys_functions import query, save_data_path\n",
    "from tqdm import notebook\n",
    "import brainbox as bb\n",
    "from ibllib.io import spikeglx\n",
    "import brainbox.behavior.wheel as wh\n",
    "from brainbox import singlecell\n",
    "from brainbox.metrics.single_units import spike_sorting_metrics\n",
    "from one.api import ONE\n",
    "import alf\n",
    "\n",
    "from ibllib.qc.camera import CameraQC\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "from collections import Counter\n",
    "\n",
    "from utils import *\n",
    "\n",
    "one = ONE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mtnn_criteria = check_mtnn_criteria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load all rs trajectories\n",
    "# trajs = query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtnn_eids = get_mtnn_eids()\n",
    "print(list(mtnn_eids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run Charles's behavioral model\n",
    "# run_exp_prevAction(mtnn_eids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = get_traj(mtnn_eids)\n",
    "#print(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ts_directory = os.path.join('.', \"new_timestamps\")\n",
    "# if not os.path.exists(ts_directory):\n",
    "#     os.makedirs(ts_directory)\n",
    "# for i in notebook.tqdm(range(len(traj))):\n",
    "#     eid = traj[i]['session']['id']\n",
    "#     get_new_timestamps(eid, 'left', ts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list, output_list, cluster_number_list, session_list, trial_number_list = load_original(mtnn_eids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_high_fr_neurons(feature, output, clusters, \n",
    "                           neuron_id_start=0, threshold1=5.0, threshold2=2.0, max_n_neurons=15):\n",
    "    #select = output.mean(1).max(1) >= threshold\n",
    "    select = np.logical_and(output.mean(1).max(1) >= threshold1, np.mean(output, axis=(1,2)) >= threshold2)\n",
    "    feature_subset = feature[select]\n",
    "    if feature_subset.shape[0] > max_n_neurons:\n",
    "        select2 = np.random.choice(np.arange(feature_subset.shape[0]), size=max_n_neurons, replace=False)\n",
    "    else:\n",
    "        select2 = np.arange(feature_subset.shape[0])\n",
    "    feature_subset = feature_subset[select2]\n",
    "    for i in range(feature_subset.shape[0]):\n",
    "        feature_subset[i,:,:,0] = neuron_id_start+i\n",
    "    \n",
    "    return feature_subset, output[select][select2], clusters[select][select2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_n_neurons = 0\n",
    "shape_list = []\n",
    "output_subset_list = []\n",
    "cluster_subset_list = []\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    feature_subset, output_subset, clusters_subset = select_high_fr_neurons(feature_list[i], \n",
    "                                                                           output_list[i],\n",
    "                                                                           cluster_number_list[i],\n",
    "                                                                           neuron_id_start=total_n_neurons,\n",
    "                                                                           threshold1=8.0,\n",
    "                                                                           threshold2=2.5,\n",
    "                                                                           max_n_neurons=15)\n",
    "    total_n_neurons += feature_subset.shape[0]\n",
    "    print('{}/{} remaining'.format(feature_subset.shape[0],feature_list[i].shape[0]))\n",
    "    print('{}/{} removed'.format(feature_list[i].shape[0]-feature_subset.shape[0],feature_list[i].shape[0]))\n",
    "    shape_list.append(feature_subset.shape)\n",
    "    output_subset_list.append(output_subset)\n",
    "    cluster_subset_list.append(clusters_subset)\n",
    "    \n",
    "    if i == 0:\n",
    "        feature_concat = feature_subset.reshape((-1,)+feature_subset.shape[-2:])\n",
    "    else:\n",
    "        feature_concat = np.concatenate((feature_concat, feature_subset.reshape((-1,)+feature_subset.shape[-2:])))\n",
    "print('feature_concat shape: {}'.format(feature_concat.shape))\n",
    "print(f'number of neurons left: {total_n_neurons}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feature(feature_concat):\n",
    "    # normalize truncated_feature: dlc features + xyz location\n",
    "    # first normalize xyz\n",
    "    x_max = feature_concat[:,:,xyz_offset].max()\n",
    "    x_min = feature_concat[:,:,xyz_offset].min()\n",
    "    y_max = feature_concat[:,:,xyz_offset+1].max()\n",
    "    y_min = feature_concat[:,:,xyz_offset+1].min()\n",
    "    z_max = feature_concat[:,:,xyz_offset+2].max()\n",
    "    z_min = feature_concat[:,:,xyz_offset+2].min()\n",
    "\n",
    "    feature_concat[:,:,xyz_offset] = 0.1 + 0.9*(feature_concat[:,:,xyz_offset] - x_min) / (x_max - x_min)\n",
    "    feature_concat[:,:,xyz_offset+1] = 0.1 + 0.9*(feature_concat[:,:,xyz_offset+1] - y_min) / (y_max - y_min)\n",
    "    feature_concat[:,:,xyz_offset+2] = 0.1 + 0.9*(feature_concat[:,:,xyz_offset+2] - z_min) / (z_max - z_min)\n",
    "\n",
    "    # next normalize dlc features\n",
    "    for i in range(stimulus_offset - paw_offset):\n",
    "        idx = paw_offset+i\n",
    "\n",
    "        feature_min = feature_concat[:,:,idx].min()\n",
    "        feature_max = feature_concat[:,:,idx].max()\n",
    "\n",
    "        feature_concat[:,:,idx] = 0.1 + 0.9*(feature_concat[:,:,idx] - feature_min) / (feature_max - feature_min)\n",
    "\n",
    "    # next normalize wheel\n",
    "    wheel_min = feature_concat[:,:,wheel_offset].min()\n",
    "    wheel_max = feature_concat[:,:,wheel_offset].max()\n",
    "\n",
    "    feature_concat[:,:,wheel_offset] = -1 + 2*(feature_concat[:,:,wheel_offset] - wheel_min) / (wheel_max - wheel_min)\n",
    "\n",
    "    # next normalize max_ptp\n",
    "    max_ptp_min = feature_concat[:,:,max_ptp_offset].min()\n",
    "    max_ptp_max = feature_concat[:,:,max_ptp_offset].max()\n",
    "\n",
    "    feature_concat[:,:,max_ptp_offset] = 0.1 + 0.9*(feature_concat[:,:,max_ptp_offset] - max_ptp_min) / (max_ptp_max - max_ptp_min)\n",
    "\n",
    "    # next normalize wf_width\n",
    "    wf_width_min = feature_concat[:,:,wf_width_offset].min()\n",
    "    wf_width_max = feature_concat[:,:,wf_width_offset].max()\n",
    "\n",
    "    feature_concat[:,:,wf_width_offset] = 0.1 + 0.9*(feature_concat[:,:,wf_width_offset] - wf_width_min) / (wf_width_max - wf_width_min)\n",
    "    \n",
    "    # noise\n",
    "    noise = np.random.normal(loc=0.0, scale=1.0, size=feature_concat.shape[:-1])\n",
    "    feature_concat[:,:,noise_offset] = noise\n",
    "    \n",
    "    return feature_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_feature = preprocess_feature(feature_concat)\n",
    "print(preprocessed_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_feature_list = []\n",
    "idx = 0\n",
    "for sh in shape_list:\n",
    "    n = sh[0]*sh[1]\n",
    "    preprocessed_feature_list.append(preprocessed_feature[idx:idx+n].reshape(sh))\n",
    "    idx += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_shape_list = []\n",
    "val_shape_list = []\n",
    "test_shape_list = []\n",
    "\n",
    "train_bool_list = []\n",
    "val_bool_list = []\n",
    "test_bool_list = []\n",
    "\n",
    "train_trial_list = []\n",
    "val_trial_list = []\n",
    "test_trial_list = []\n",
    "\n",
    "train_feature = []\n",
    "val_feature = []\n",
    "test_feature = []\n",
    "\n",
    "train_output = []\n",
    "val_output = []\n",
    "test_output = []\n",
    "\n",
    "for i in notebook.tqdm(range(len(traj))):\n",
    "    try:\n",
    "        print(session_list[i]['session']['id'])\n",
    "    except:\n",
    "        print(session_list[i].tolist()['session']['id'])\n",
    "    \n",
    "    n_trials = preprocessed_feature_list[i].shape[1]\n",
    "    n_test = int(n_trials*0.2)\n",
    "    n_train = int((n_trials-n_test)*0.8)\n",
    "    n_val = n_trials - n_train - n_test\n",
    "    \n",
    "    sh = shape_list[i]\n",
    "    train_shape_list.append((sh[0],n_train,)+sh[-2:])\n",
    "    val_shape_list.append((sh[0],n_val,)+sh[-2:])\n",
    "    test_shape_list.append((sh[0],n_test,)+sh[-2:])\n",
    "    \n",
    "    test_idx = np.random.choice(np.arange(n_trials), size=n_test, replace=False)\n",
    "    test_bool = np.zeros(n_trials).astype(bool)\n",
    "    test_bool[test_idx] = True\n",
    "    \n",
    "    train_idx = np.random.choice(np.arange(n_trials)[~test_bool], size=n_train, replace=False)\n",
    "    train_bool = np.zeros(n_trials).astype(bool)\n",
    "    train_bool[train_idx] = True\n",
    "    \n",
    "    val_bool = np.zeros(n_trials).astype(bool)\n",
    "    val_bool[~np.logical_or(test_bool, train_bool)] = True\n",
    "    \n",
    "    train_bool_list.append(train_bool)\n",
    "    val_bool_list.append(val_bool)\n",
    "    test_bool_list.append(test_bool)\n",
    "    \n",
    "    train_trial_list.append(trial_number_list[i][train_bool])\n",
    "    val_trial_list.append(trial_number_list[i][val_bool])\n",
    "    test_trial_list.append(trial_number_list[i][test_bool])\n",
    "    \n",
    "    train_feature.append(preprocessed_feature_list[i][:,train_bool].reshape((-1,)+sh[-2:]))\n",
    "    val_feature.append(preprocessed_feature_list[i][:,val_bool].reshape((-1,)+sh[-2:]))\n",
    "    test_feature.append(preprocessed_feature_list[i][:,test_bool].reshape((-1,)+sh[-2:]))\n",
    "    \n",
    "    train_output.append(output_subset_list[i][:,train_bool].reshape(-1, sh[-2]))\n",
    "    val_output.append(output_subset_list[i][:,val_bool].reshape(-1, sh[-2]))\n",
    "    test_output.append(output_subset_list[i][:,test_bool].reshape(-1, sh[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_path = save_data_path(figure='figure8').joinpath('mtnn_data')\n",
    "save_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "save_path_train = save_data_path(figure='figure8').joinpath('mtnn_data/train')\n",
    "save_path_train.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "save_path_val = save_data_path(figure='figure8').joinpath('mtnn_data/validation')\n",
    "save_path_val.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "save_path_test = save_data_path(figure='figure8').joinpath('mtnn_data/test')\n",
    "save_path_test.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.save(save_path_train.joinpath('shape.npy'), np.asarray(train_shape_list))\n",
    "np.save(save_path_val.joinpath('shape.npy'), np.asarray(val_shape_list))\n",
    "np.save(save_path_test.joinpath('shape.npy'), np.asarray(test_shape_list))\n",
    "\n",
    "np.save(save_path_train.joinpath('bool.npy'), np.asarray(train_bool_list, dtype=object))\n",
    "np.save(save_path_val.joinpath('bool.npy'), np.asarray(val_bool_list, dtype=object))\n",
    "np.save(save_path_test.joinpath('bool.npy'), np.asarray(test_bool_list, dtype=object))\n",
    "\n",
    "np.save(save_path_train.joinpath('trials.npy'), np.asarray(train_trial_list, dtype=object))\n",
    "np.save(save_path_val.joinpath('trials.npy'), np.asarray(val_trial_list, dtype=object))\n",
    "np.save(save_path_test.joinpath('trials.npy'), np.asarray(test_trial_list, dtype=object))\n",
    "\n",
    "np.save(save_path_train.joinpath('feature.npy'), np.concatenate(train_feature))\n",
    "np.save(save_path_val.joinpath('feature.npy'), np.concatenate(val_feature))\n",
    "np.save(save_path_test.joinpath('feature.npy'), np.concatenate(test_feature))\n",
    "\n",
    "np.save(save_path_train.joinpath('output.npy'), np.concatenate(train_output))\n",
    "np.save(save_path_val.joinpath('feature.npy'), np.concatenate(val_output))\n",
    "np.save(save_path_test.joinpath('feature.npy'), np.concatenate(test_output))\n",
    "\n",
    "np.save(save_path.joinpath('session_info.npy'), np.asarray(session_list))\n",
    "np.save(save_path.joinpath('clusters.npy'), np.asarray(cluster_subset_list, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv-updated",
   "language": "python",
   "name": "iblenv-updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
